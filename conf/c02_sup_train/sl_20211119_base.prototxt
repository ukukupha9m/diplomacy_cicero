# Baseline BC/SL model for learning policy and value from human data.
# This file is provided for posterity as a record of various training parameters
# Actually running this configuration to train a model requires obtaining the data,
# as well as some nontrivial preprocessing of that data.
includes { path: "launcher/slurm_8gpus.prototxt"; mount: "launcher" }
train {
    dataset_params: {
        train_set_path: "PATH_TO_TRAIN.data";
        val_set_path: "PATH_TO_VAL.data";
        metadata_path: "PATH_TO_METADATA.json";
        debug_only_opening_phase: false;
        only_with_min_final_score: 0;
        min_rating_percentile: 0.5;
        min_total_games: 5;
        exclude_n_holds: 3;
    }

    batch_size: 500;
    lr: 0.002;
    lr_decay: 0.99;
    clip_grad_norm: 0.5;
    checkpoint: "./checkpoint.pth";
    teacher_force: 1.0;
    lstm_layers: 2;
    lstm_dropout: 0.30;
    inter_emb_size: 112;
    num_epochs: 400;
    debug_no_mp: false;
    skip_validation: false;
    write_jsonl: true;
    value_decoder_init_scale: 0.01;
    value_decoder_clip_grad_norm: 0.5;
    value_decoder_activation: "gelu";
    value_decoder_use_weighted_pool: true;
    value_loss_weight: 0.7;
    featurize_output: true;
    featurize_prev_orders: true;
    relfeat_output: true;
    value_softmax: true;
    encoder {
      transformer: {
        num_heads: 8;
        ff_channels: 224;
        num_blocks: 10;
        dropout: 0.30;
        activation: "gelu"
      }
    };
    use_default_requeue: true;

    input_version: 3;
    training_permute_powers: true;
    use_v2_base_strategy_model: true;
    num_scoring_systems: 2;
    warmup_epochs: 10

    launcher {
      slurm {
        num_gpus: 32
        partition: "learnaccel"
      }
    }
}
